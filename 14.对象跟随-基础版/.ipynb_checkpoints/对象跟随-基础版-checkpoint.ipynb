{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../图片数据/logo.png\" alt=\"Header\" style=\"width: 800px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@Copyright (C): 2010-2019, Shenzhen Yahboom Tech  \n",
    "@Author: Malloy.Yuan  \n",
    "@Date: 2019-07-17 10:10:02  \n",
    "@LastEditors: Malloy.Yuan  \n",
    "@LastEditTime: 2019-09-17 17:54:19  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对象跟随- 基础版\n",
    "\n",
    "在这个历程中，我们将展示如何使用JetBot跟踪对象!\n",
    "我们将使用一个在[COCO dataset](http://cocodataset.org)上训练过的预训练神经网络来检测90个不同的常见对象。这些包括\n",
    "\n",
    "* Person(人) (index 0)\n",
    "* Cup(杯子) (index 47)\n",
    "\n",
    "不管怎样，我们开始吧。首先，我们先导入``ObjectDetector``类，它使用我们预先训练的SSD引擎。还有许多其他类(你可以检查[这个文件](https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_complete_label_map.pbt)以获得类索引的完整列表)。该模型来自[TensorFlow对象检测API](https://github.com/tensorflow/models/tree/master/research/object_detection)，该API还为定制任务的对象检测器培训提供实用程序!一旦模型被训练，我们使用NVIDIA TensorRT对Jetson Nano进行优化。这使得网络非常快，能够实时控制Jetbot!但是，我们不会在这个笔记本中运行所有的培训和优化步骤。不管怎样，我们开始吧。首先，我们要导入“ObjectDetector”类，该类使用我们预先训练的SSD引擎。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 单张相机图像检测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from jetbot import ObjectDetector\n",
    "\n",
    "model = ObjectDetector('ssd_mobilenet_v2_coco.engine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在内部，' ' ObjectDetector ' '类使用TensorRT Python API来执行我们提供的引擎。它还负责对神经网络的输入进行预处理，以及对检测到的对象进行解析。目前，它只适用于使用``jetbotssd_tensorrt``包创建的引擎。该包具有将模型从TensorFlow对象检测API转换为优化的TensorRT引擎的实用程序。\n",
    "接下来，让我们初始化相机。我们的检测器需要300x300像素的输入，所以我们会在创建相机时设置这个。\n",
    "\n",
    ">在内部，Camera类使用GStreamer来利用Jetson Nano的图像信号处理器(ISP),获取图像的速度是非常可观的.\n",
    "\n",
    ">并且还从CPU卸载了大量的大小计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import Camera\n",
    "\n",
    "camera = Camera.instance(width=300, height=300)\n",
    "#camera = Camera.instance(width=220, height=220, fps=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，让我们使用一些摄像机输入来执行我们的网络。在默认情况下，``ObjectDetector``类期望相机生成的格式为``bgr8``。\n",
    "然而，如果输入格式不同，可以覆盖默认的预处理函数。\n",
    "\n",
    "如果相机的视场中有任何COCO对象，它们现在应该存储在``detections``变量中。\n",
    "### 在文本区域显示检测的对象\n",
    "\n",
    "我们将使用下面的代码打印出检测到的对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 62, 'confidence': 0.7961339950561523, 'bbox': [0.007378697395324707, 0.022159874439239502, 0.37059885263442993, 0.6796097159385681]}]]\n"
     ]
    }
   ],
   "source": [
    "detections = model(camera.value)\n",
    "\n",
    "print(detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d45c5df75c846648414ab955a4fc055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value=\"[[{'label': 62, 'confidence': 0.7961339950561523, 'bbox': [0.007378697395324707, 0.02215987443…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "import ipywidgets.widgets as widgets\n",
    "\n",
    "detections_widget = widgets.Textarea()\n",
    "detections_widget.value = str(detections)\n",
    "display(detections_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您应该看到每个图像中检测到的每个对象的标签、置信度和边框位置。在这个例子中只有一个图像(我们的相机)。\n",
    "\n",
    "要打印第一张图像中检测到的第一个对象，我们可以调用以下命令\n",
    "\n",
    ">如果没有检测到对象，这可能会抛出一个错误"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 62, 'confidence': 0.7961339950561523, 'bbox': [0.007378697395324707, 0.022159874439239502, 0.37059885263442993, 0.6796097159385681]}\n"
     ]
    }
   ],
   "source": [
    "image_number = 0\n",
    "object_number = 0\n",
    "\n",
    "print(detections[image_number][object_number])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 如果没有检测到对象，这里可能会抛出一个错误"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 控制机器人跟随中心物体\n",
    "\n",
    "现在，我们希望机器人跟随指定类的对象。为此，我们将做以下工作\n",
    "\n",
    "1.  检测与指定类匹配的对象\n",
    "2.  选择距离相机视野中心最近的物体，这是目标物体\n",
    "3.  引导机器人向目标物体移动，否则会产生漂移\n",
    "\n",
    "我们还将创建一些小部件，用于控制目标对象标签、机器人速度和转弯增益，根据目标对象和机器人视野中心之间的距离来控制机器人转弯的速度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "mean = 255.0 * np.array([0.485, 0.456, 0.406])\n",
    "stdev = 255.0 * np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "normalize = torchvision.transforms.Normalize(mean, stdev)\n",
    "\n",
    "def preprocess(camera_value):\n",
    "    global device, normalize\n",
    "    x = camera_value\n",
    "    #图片缩放至224,224对比224,244的避障模型\n",
    "    x = cv2.resize(x, (224, 224))\n",
    "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "    x = x.transpose((2, 0, 1))\n",
    "    x = torch.from_numpy(x).float()\n",
    "    x = normalize(x)\n",
    "    x = x.to(device)\n",
    "    x = x[None, ...]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建驱动电机的robot实例."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import Robot\n",
    "\n",
    "robot = Robot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，让我们显示所有控件小部件，并将网络执行功能连接到相机更新。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe020a6c5f4b4979b498fa8ebdd6f4a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Image(value=b'', format='jpeg', height='300', width='300'),)), IntText(value=1, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from jetbot import bgr8_to_jpeg\n",
    "\n",
    "image_widget = widgets.Image(format='jpeg', width=300, height=300)\n",
    "#将下面这条语句的value值改为检测目标物体的对象值，取值范围0-99，例如跟随对象是Person(人) (index 1)，需要将value的值改为1\n",
    "label_widget = widgets.IntText(value=1, description='tracked label')\n",
    "speed_widget = widgets.FloatSlider(value=0.5, min=0.0, max=1.0, description='speed')\n",
    "turn_gain_widget = widgets.FloatSlider(value=0.8, min=0.0, max=2.0, description='turn gain')\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HBox([image_widget]),\n",
    "    label_widget,\n",
    "    speed_widget,\n",
    "    turn_gain_widget\n",
    "]))\n",
    "\n",
    "width = int(image_widget.width)\n",
    "height = int(image_widget.height)\n",
    "\n",
    "def detection_center(detection):\n",
    "    \"\"\"计算对象的中心x、y坐标\"\"\"\n",
    "    bbox = detection['bbox']\n",
    "    center_x = (bbox[0] + bbox[2]) / 2.0 - 0.5\n",
    "    center_y = (bbox[1] + bbox[3]) / 2.0 - 0.5\n",
    "    return (center_x, center_y)\n",
    "    \n",
    "def norm(vec):\n",
    "    \"\"\"计算二维向量的长度\"\"\"\n",
    "    return np.sqrt(vec[0]**2 + vec[1]**2)\n",
    "\n",
    "def closest_detection(detections):\n",
    "    \"\"\"查找最接近图像中心的检测\"\"\"\n",
    "    closest_detection = None\n",
    "    for det in detections:\n",
    "        center = detection_center(det)\n",
    "        if closest_detection is None:\n",
    "            closest_detection = det\n",
    "        elif norm(detection_center(det)) < norm(detection_center(closest_detection)):\n",
    "            closest_detection = det\n",
    "    return closest_detection\n",
    "        \n",
    "def execute(change):\n",
    "    image = change['new']\n",
    "    # 计算所有检测到的对象\n",
    "    detections = model(image)\n",
    "    \n",
    "    # 在图像上绘制所有检测\n",
    "    for det in detections[0]:\n",
    "        bbox = det['bbox']\n",
    "        cv2.rectangle(image, (int(width * bbox[0]), int(height * bbox[1])), (int(width * bbox[2]), int(height * bbox[3])), (255, 0, 0), 2)\n",
    "    \n",
    "    # 选择匹配所选类标签的检测\n",
    "    matching_detections = [d for d in detections[0] if d['label'] == int(label_widget.value)]\n",
    "    \n",
    "    # 让检测最接近视野中心，并绘制它\n",
    "    det = closest_detection(matching_detections)\n",
    "    if det is not None:\n",
    "        bbox = det['bbox']\n",
    "        cv2.rectangle(image, (int(width * bbox[0]), int(height * bbox[1])), (int(width * bbox[2]), int(height * bbox[3])), (0, 255, 0), 5)\n",
    "    \n",
    "    # 如果没有检测到目标，则继续前进\n",
    "    if det is None:\n",
    "        pass\n",
    "        robot.stop()\n",
    "        # robot.forward(float(speed_widget.value))\n",
    "        \n",
    "    # 有的话就控制Jetbot去跟随设定的对象\n",
    "    else:\n",
    "        # 将机器人向前移动，并控制成比例的目标与中心的x距离\n",
    "        center = detection_center(det)\n",
    "        robot.set_motors(\n",
    "            float(speed_widget.value + turn_gain_widget.value * center[0]),\n",
    "            float(speed_widget.value - turn_gain_widget.value * center[0])\n",
    "        )\n",
    "    \n",
    "    # 更新图像显示至小部件\n",
    "    image_widget.value = bgr8_to_jpeg(image)\n",
    "    \n",
    "execute({'new': camera.value})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调用下面的块将执行函数连接到每个摄像机帧更新。\n",
    "> ## 注意注意:以下函数由于之前通过画面变化来调用检测函数延迟比较大， 现在暂时采用循环不断调用检测函数提高画面流畅度，以及提高识别效果，由于在主线程中不断循环检测会导致卡死在这里并且使得jupyterlab的所有按键失灵，所以采用将不断检测的死循环放到一个子线程1里面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#camera.unobserve_all()\n",
    "#camera.observe(execute, names='value')\n",
    "\n",
    "import threading\n",
    "import inspect\n",
    "import ctypes\n",
    "'''以下为定义关闭线程函数'''\n",
    "def _async_raise(tid, exctype):\n",
    "  \"\"\"raises the exception, performs cleanup if needed\"\"\"\n",
    "  tid = ctypes.c_long(tid)\n",
    "  if not inspect.isclass(exctype):\n",
    "    exctype = type(exctype)\n",
    "  res = ctypes.pythonapi.PyThreadState_SetAsyncExc(tid, ctypes.py_object(exctype))\n",
    "  if res == 0:\n",
    "    raise ValueError(\"invalid thread id\")\n",
    "  elif res != 1:\n",
    "    # \"\"\"if it returns a number greater than one, you're in trouble,\n",
    "    # and you should call it again with exc=NULL to revert the effect\"\"\"\n",
    "    ctypes.pythonapi.PyThreadState_SetAsyncExc(tid, None)\n",
    "    raise SystemError(\"PyThreadState_SetAsyncExc failed\")\n",
    "def stop_thread(thread):\n",
    "  _async_raise(thread.ident, SystemExit)\n",
    "'''线程1的函数，里面不断调用检测函数'''\n",
    "def test():\n",
    "    while True:\n",
    "        execute({'new': camera.value})        \n",
    "\n",
    "        \n",
    "thread1 = threading.Thread(target=test)\n",
    "thread1.setDaemon(False)\n",
    "thread1.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果机器人没有被阻挡，你应该可以看到蓝色的方框围绕着被探测到的物体,目标对象(机器人跟随的对象)将显示为绿色。\n",
    "当目标被发现时，机器人应该转向目标。\n",
    "你可以调用下面的代码块来手动断开与摄像机的处理并停止机器人。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "stop_thread(thread1)\n",
    "camera.unobserve_all()\n",
    "time.sleep(1.0)\n",
    "robot.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
